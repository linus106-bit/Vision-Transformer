{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "B6saS6tUtsch"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Patch Embedding\n",
        "# 0. Patch Embedding Variables\n",
        "p = 4 # patch\n",
        "w = 32 # width\n",
        "h = 32 # height\n",
        "c = 3 # channel\n",
        "b = 128 # batch\n",
        "d = 128 # Dim of patched embeddings\n",
        "cls = 10 # Class token size\n",
        "L = 12 # Transformer block size\n",
        "head_num = 8 # attention heads\n",
        "n = w//p"
      ],
      "metadata": {
        "id": "Hf3wuQVktvWc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = b\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1g7cQbUuSUd",
        "outputId": "9d0c7c69-88dd-4915-ba80-c02e33286d4e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainable Linear Projection이 필요\n",
        "# nn.Module로 구성\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.projection = nn.Linear(p*p*c, d) # These image patch vectors are now encoded using a linear transformation. Fixed size `d`\n",
        "\n",
        "    def patchify(self,img):\n",
        "      # Divide to patch\n",
        "      patched_img = img.view(b,c,h//p,p,w//p,p) # 이미지 1개당 N*N개 패치가 나오고, 패치 하나의 이미지는 P*P*C\n",
        "      patched_img = patched_img.transpose(3,4)\n",
        "      patched_img = patched_img.transpose(1,3)\n",
        "      patched_img = patched_img.transpose(1,2)\n",
        "      patched_img = patched_img.reshape(b,n*n,p*p*c)\n",
        "      return patched_img\n",
        "    def class_emb(self, patch):\n",
        "      x_class = nn.Parameter(torch.randn(b,1,d)).to(device)\n",
        "      with_class = torch.cat((x_class, patch), dim = 1)\n",
        "      # print(\"with class embedding : \", with_class.shape)\n",
        "      return with_class\n",
        "\n",
        "    def position_emb(self, class_patch):\n",
        "      pos_emb = nn.Parameter(torch.randn(b,n*n+1,d)).to(device)\n",
        "      with_class_pos = class_patch + pos_emb # 이게 맞나? 그냥 더하는게?\n",
        "      # print(\"with class & positional embedding : \", with_class_pos.shape)\n",
        "      return with_class_pos\n",
        "\n",
        "    def forward(self, x):\n",
        "      patched_ = self.patchify(x)\n",
        "      patched_ = self.projection(patched_)\n",
        "      patched_ = self.class_emb(patched_)\n",
        "      patched_ = self.position_emb(patched_)\n",
        "      return patched_"
      ],
      "metadata": {
        "id": "8zVvDkqouCUP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fNs7ITMQtlyZ"
      },
      "outputs": [],
      "source": [
        "# Transformer\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Attention, self).__init__()\n",
        "    self.w_q = nn.Parameter(torch.randn(d, n*n+1))\n",
        "    self.w_k = nn.Parameter(torch.randn(d, n*n+1))\n",
        "    self.w_v = nn.Parameter(torch.randn(d, n*n+1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # W_q,W_k,W_v 를 정의\n",
        "    q = x @ self.w_q\n",
        "    k = x @ self.w_k\n",
        "    v = x @ self.w_v\n",
        "    # QK^T를 만들기\n",
        "    qk_T = q @ k.T\n",
        "    # k의 차원 : D (Latent vector)\n",
        "    qk_T = qk_T / d\n",
        "    soft_ = nn.Softmax(dim = 0)\n",
        "    attention_ = soft_(qk_T)\n",
        "    # print(attention_.shape, v.shape)\n",
        "    ret = attention_ @ v\n",
        "    return ret\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.attn = Attention()\n",
        "    self.w_o = nn.Parameter(torch.randn(head_num*(n*n+1), d))\n",
        "  def forward(self,x):\n",
        "    # Head의 Concat이 필요\n",
        "    head_list = []\n",
        "    for h in range(head_num):\n",
        "\n",
        "      x_h = self.attn(x)\n",
        "      head_list.append(x_h)\n",
        "    ret = torch.cat(head_list, dim =1)\n",
        "    ret = ret @ self.w_o\n",
        "    return ret\n",
        "\n",
        "class VisionTransformerBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VisionTransformerBlock, self).__init__()\n",
        "    self.msa = MultiHeadAttention()\n",
        "    self.bn1 = nn.LayerNorm(d) # Size of BatchNorm1d is the input's size\n",
        "    self.bn2 = nn.LayerNorm(d) # Size of BatchNorm1d is the input's size\n",
        "    self.mlp = nn.Linear(d,d)\n",
        "  def forward(self, x):\n",
        "    # Batch Norm 1d\n",
        "    x = self.bn1(x)\n",
        "    # Multi-head Attention (Done)\n",
        "    x_attn = self.msa(x)\n",
        "    # print(x_attn.shape, x.shape)\n",
        "    # Residual connections\n",
        "    x_attn = x_attn + x\n",
        "    # Norm\n",
        "    out = self.bn2(x_attn)\n",
        "    # MLP\n",
        "    out = self.mlp(x_attn)\n",
        "    # Concat\n",
        "    out = out + x_attn\n",
        "    # print(out.shape)\n",
        "    return out\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VisionTransformer, self).__init__()\n",
        "    self.vit = nn.ModuleList([VisionTransformerBlock()\n",
        "                              for _ in range(L)])\n",
        "    # Full Connected Layer\n",
        "    self.mlp = nn.Sequential(\n",
        "            nn.LayerNorm(d),\n",
        "            nn.Linear(d, cls)\n",
        "        )\n",
        "    self.pe = PositionalEmbedding()\n",
        "  def forward(self,x):\n",
        "    pe_out = self.pe(x)\n",
        "    # Seqeuence L 반복\n",
        "    # ViT가 계속 업데이트 되야되는데 ..\n",
        "    outputs = []\n",
        "    for d in pe_out:\n",
        "      # print(d.shape)\n",
        "      for layer in self.vit:\n",
        "        d = layer(d)\n",
        "      # print(d.shape)\n",
        "      outputs.append(d)\n",
        "      # 각 이미지에 대한 output을 의미해야되는데\n",
        "      # label이 0,1이 아니라 1~10으로 구성이 되어 있다.\n",
        "    outputs = torch.stack(outputs,dim = 0).to(device)\n",
        "    out = self.mlp(outputs[:,0]) # 처음 토큰이 class token 이니까\n",
        "    return out.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(dataloader, model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "    n = len(dataloader)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for data in dataloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs, _ = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        loss_result = running_loss / n\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    model.train()\n",
        "    return acc, loss_result"
      ],
      "metadata": {
        "id": "RVSmO4dhvzFm"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "ViT = VisionTransformer()\n",
        "ViT.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(ViT.parameters(), lr=0.001, momentum=0.9)\n",
        "# 지금은 patch를 1D로 만들고, cls, pos 를 붙임\n",
        "# patch , cls, pos를 붙인 다음에\n",
        "\n",
        "ViT.train()\n",
        "# print(ViT)\n",
        "n = len(train_loader)\n",
        "for epoch in range(10):\n",
        "  running_loss = 0\n",
        "  for img, label in tqdm(train_loader):\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    out = ViT(img)\n",
        "    label_f32 = label.type('torch.LongTensor').to(device)\n",
        "    # print(out.dtype, label_f32.dtype)\n",
        "    loss = criterion(out, label_f32)\n",
        "\n",
        "    # loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() #retain_graph=True\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss = running_loss / n\n",
        "  val_acc, val_loss = accuracy(test_loader, ViT)\n",
        "  # if epoch % 5 == 0:\n",
        "  print('[%d] train loss: %.3f, validation loss: %.3f, validation acc %.2f %%' % (epoch, train_loss, val_loss, val_acc))\n",
        "  torch.save(ViT, '/content/drive/MyDrive/Vision-Transformer/model_new.pth')\n",
        "  # ViT.eval()\n",
        "  # test_loss = 0.0\n",
        "  # correct = 0\n",
        "\n",
        "  # with torch.no_grad():\n",
        "  #     for images, labels in test_loader:\n",
        "\n",
        "  #         images = images.to(device)\n",
        "  #         labels = labels.to(device)\n",
        "\n",
        "  #         outputs = ViT(images)\n",
        "  #         predicted = torch.max(outputs, 1)[1]\n",
        "  #         loss = criterion(outputs, labels)\n",
        "\n",
        "  #         test_loss += loss.item()\n",
        "  #         correct += (labels == predicted).sum()\n",
        "  # # Test Loss Logging\n",
        "  # print(\n",
        "  #     f\"epoch {epoch+1} - test loss: {test_loss / len(test_loader):.4f}\"\n",
        "  # )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "chDG9cMcuNo_",
        "outputId": "b3bf4b4d-6ce3-4502-af22-117ee36b91a6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[10, 25000000, 48]' is invalid for input of size 30720",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e9013083eca6>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mlabel_f32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.LongTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# print(out.dtype, label_f32.dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-9c2beddff337>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionalEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mpe_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Seqeuence L 반복\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# ViT가 계속 업데이트 되야되는데 ..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-0c40c7d31bac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mpatched_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m       \u001b[0mpatched_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatched_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0mpatched_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatched_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-0c40c7d31bac>\u001b[0m in \u001b[0;36mpatchify\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mpatched_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatched_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mpatched_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatched_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mpatched_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatched_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mpatched_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclass_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[10, 25000000, 48]' is invalid for input of size 30720"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YhxY3HxWvcmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/drive/MyDrive/Vision-Transformer/model.pth')\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        print(outputs)\n",
        "        print(labels)\n",
        "        break"
      ],
      "metadata": {
        "id": "eV48yLuAV1_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea16c68-2cd3-4218-fe4c-77661660eba9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3103,  0.1640, -0.4686, -0.0148, -0.2606, -0.3470, -0.6482, -0.1207,\n",
            "          0.2620,  0.3200],\n",
            "        [ 0.0631,  0.6368,  0.1217,  0.0908, -0.1239,  0.2969, -0.2306, -0.2415,\n",
            "          0.4690,  0.3366],\n",
            "        [-0.0240, -0.1394, -0.0156,  0.2074,  0.1959,  0.0371,  0.0841,  0.0255,\n",
            "         -0.1048,  0.1100],\n",
            "        [ 0.0179, -1.1196,  0.8682,  0.6601,  0.9979,  0.5115,  0.4984,  0.5505,\n",
            "         -0.2318, -0.9103],\n",
            "        [ 0.2933,  0.1393, -0.4507, -0.0085, -0.2379, -0.3338, -0.6183, -0.1007,\n",
            "          0.2344,  0.2979],\n",
            "        [ 0.1185,  0.8088,  0.0116,  0.0674, -0.2360,  0.2458, -0.3930, -0.3428,\n",
            "          0.5987,  0.5082],\n",
            "        [ 0.0695,  0.6647,  0.1017,  0.0847, -0.1439,  0.2872, -0.2572, -0.2616,\n",
            "          0.4855,  0.3657],\n",
            "        [-0.3389, -0.2026,  0.6575,  0.4197,  0.8786,  0.8239,  0.9859,  0.4691,\n",
            "         -0.7461, -0.3810],\n",
            "        [ 0.2837,  0.1157, -0.4415, -0.0087, -0.2257, -0.3303, -0.5978, -0.0878,\n",
            "          0.2106,  0.2784],\n",
            "        [ 0.3209,  0.1799, -0.4799, -0.0188, -0.2750, -0.3552, -0.6669, -0.1334,\n",
            "          0.2795,  0.3343]], device='cuda:0')\n",
            "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}