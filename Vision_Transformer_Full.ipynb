{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B6saS6tUtsch"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hf3wuQVktvWc"
      },
      "outputs": [],
      "source": [
        "# 1. Patch Embedding\n",
        "# 0. Patch Embedding Variables\n",
        "p = 4 # patch\n",
        "w = 32 # width\n",
        "h = 32 # height\n",
        "c = 3 # channel\n",
        "b = 128 # batch\n",
        "d = 128 # Dim of patched embeddings\n",
        "cls = 10 # Class token size\n",
        "L = 8 # Transformer block size\n",
        "\n",
        "n = w//p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1g7cQbUuSUd",
        "outputId": "418d6725-5ba1-42e2-e0a1-6b72758b83b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Import Dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8zVvDkqouCUP"
      },
      "outputs": [],
      "source": [
        "# Trainable Linear Projection이 필요\n",
        "# nn.Module로 구성\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.projection = nn.Linear(p*p*c, d) # These image patch vectors are now encoded using a linear transformation. Fixed size `d`\n",
        "\n",
        "    def patchify(self,img):\n",
        "      # Divide to patch\n",
        "      patched_img = img.view(b,c,h//p,p,w//p,p) # 이미지 1개당 N*N개 패치가 나오고, 패치 하나의 이미지는 P*P*C\n",
        "      patched_img = patched_img.transpose(3,4)\n",
        "      patched_img = patched_img.transpose(1,3)\n",
        "      patched_img = patched_img.transpose(1,2)\n",
        "      patched_img = patched_img.reshape(b,n*n,p*p*c)\n",
        "      return patched_img\n",
        "    def class_emb(self, patch):\n",
        "      x_class = nn.Parameter(torch.randn(b,1,d)).to(device)\n",
        "      with_class = torch.cat((x_class, patch), dim = 1)\n",
        "      # print(\"with class embedding : \", with_class.shape)\n",
        "      return with_class\n",
        "\n",
        "    def position_emb(self, class_patch):\n",
        "      pos_emb = nn.Parameter(torch.randn(b,n*n+1,d)).to(device)\n",
        "      with_class_pos = class_patch + pos_emb # 이게 맞나? 그냥 더하는게?\n",
        "      # print(\"with class & positional embedding : \", with_class_pos.shape)\n",
        "      return with_class_pos\n",
        "\n",
        "    def forward(self, x):\n",
        "      patched_ = self.patchify(x)\n",
        "      patched_ = self.projection(patched_)\n",
        "      patched_ = self.class_emb(patched_)\n",
        "      patched_ = self.position_emb(patched_)\n",
        "      return patched_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fNs7ITMQtlyZ"
      },
      "outputs": [],
      "source": [
        "# Transformer\n",
        "\n",
        "head_num = 8 # attention heads\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Attention, self).__init__()\n",
        "    self.w_q = nn.Parameter(torch.randn(d, n*n+1))\n",
        "    self.w_k = nn.Parameter(torch.randn(d, n*n+1))\n",
        "    self.w_v = nn.Parameter(torch.randn(d, n*n+1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # W_q,W_k,W_v 를 정의\n",
        "    q = x @ self.w_q\n",
        "    k = x @ self.w_k\n",
        "    v = x @ self.w_v\n",
        "    # QK^T를 만들기\n",
        "    print(q.shape, k.shape, v.shape)\n",
        "    qk_T = q @ k.T\n",
        "    # k의 차원 : D (Latent vector)\n",
        "    qk_T = qk_T / d\n",
        "    soft_ = nn.Softmax(dim = 0)\n",
        "    attention_ = soft_(qk_T)\n",
        "    # print(attention_.shape, v.shape)\n",
        "    ret = attention_ @ v\n",
        "    return ret\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.attn = Attention()\n",
        "    self.w_o = nn.Parameter(torch.randn(head_num*(n*n+1), d))\n",
        "  def forward(self,x):\n",
        "    # Head의 Concat이 필요\n",
        "    head_list = []\n",
        "    for h in range(head_num):\n",
        "\n",
        "      x_h = self.attn(x)\n",
        "      head_list.append(x_h)\n",
        "    ret = torch.cat(head_list, dim =1)\n",
        "    ret = ret @ self.w_o\n",
        "    return ret\n",
        "\n",
        "class VisionTransformerBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VisionTransformerBlock, self).__init__()\n",
        "    self.msa = MultiHeadAttention()\n",
        "    self.bn1 = nn.LayerNorm(d) # Size of BatchNorm1d is the input's size\n",
        "    self.bn2 = nn.LayerNorm(d) # Size of BatchNorm1d is the input's size\n",
        "    self.mlp = nn.Linear(d,d)\n",
        "  def forward(self, x):\n",
        "    # Batch Norm 1d\n",
        "    x = self.bn1(x)\n",
        "    # Multi-head Attention (Done)\n",
        "    x_attn = self.msa(x)\n",
        "    # print(x_attn.shape, x.shape)\n",
        "    # Residual connections\n",
        "    x_attn = x_attn + x\n",
        "    # Norm\n",
        "    out = self.bn2(x_attn)\n",
        "    # MLP\n",
        "    out = self.mlp(x_attn)\n",
        "    # Concat\n",
        "    out = out + x_attn\n",
        "    # print(out.shape)\n",
        "    return out\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VisionTransformer, self).__init__()\n",
        "    self.vit = nn.ModuleList([VisionTransformerBlock()\n",
        "                              for _ in range(L)])\n",
        "    # Full Connected Layer\n",
        "    self.mlp = nn.Sequential(\n",
        "            nn.LayerNorm(d),\n",
        "            nn.Linear(d, cls)\n",
        "        )\n",
        "    self.pe = PositionalEmbedding()\n",
        "  def forward(self,x):\n",
        "    pe_out = self.pe(x)\n",
        "    # Seqeuence L 반복\n",
        "    # ViT가 계속 업데이트 되야되는데 ..\n",
        "    outputs = []\n",
        "    for d in pe_out:\n",
        "      # print(d.shape)\n",
        "      for layer in self.vit:\n",
        "        d = layer(d)\n",
        "      # print(d.shape)\n",
        "      outputs.append(d)\n",
        "      # 각 이미지에 대한 output을 의미해야되는데\n",
        "      # label이 0,1이 아니라 1~10으로 구성이 되어 있다.\n",
        "    outputs = torch.stack(outputs,dim = 0).to(device)\n",
        "    out = self.mlp(outputs[:,0])\n",
        "    return out.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chDG9cMcuNo_",
        "outputId": "89de55f0-9d6b-4740-d84f-422866439b2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "shape '[128, 3, 8, 4, 8, 4]' is invalid for input of size 30720",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m label_f32 \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.LongTensor\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(out.dtype, label_f32.dtype)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[9], line 79\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 79\u001b[0m   pe_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m   \u001b[38;5;66;03m# Seqeuence L 반복\u001b[39;00m\n\u001b[1;32m     81\u001b[0m   \u001b[38;5;66;03m# ViT가 계속 업데이트 되야되는데 ..\u001b[39;00m\n\u001b[1;32m     82\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mPositionalEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 29\u001b[0m   patched_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatchify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m   patched_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(patched_)\n\u001b[1;32m     31\u001b[0m   patched_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_emb(patched_)\n",
            "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mPositionalEmbedding.patchify\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpatchify\u001b[39m(\u001b[38;5;28mself\u001b[39m,img):\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;66;03m# Divide to patch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m   patched_img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 이미지 1개당 N*N개 패치가 나오고, 패치 하나의 이미지는 P*P*C\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   patched_img \u001b[38;5;241m=\u001b[39m patched_img\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     12\u001b[0m   patched_img \u001b[38;5;241m=\u001b[39m patched_img\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[128, 3, 8, 4, 8, 4]' is invalid for input of size 30720"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "ViT = VisionTransformer()\n",
        "ViT.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(ViT.parameters(), lr=0.001, momentum=0.9)\n",
        "# 지금은 patch를 1D로 만들고, cls, pos 를 붙임\n",
        "# patch , cls, pos를 붙인 다음에\n",
        "\n",
        "ViT.train()\n",
        "# print(ViT)\n",
        "for epoch in range(2):\n",
        "  for img, label in tqdm(train_loader):\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    out = ViT(img)\n",
        "    label_f32 = label.type('torch.LongTensor').to(device)\n",
        "    # print(out.dtype, label_f32.dtype)\n",
        "    loss = criterion(out, label_f32)\n",
        "\n",
        "    # loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() #retain_graph=True\n",
        "    optimizer.step()\n",
        "\n",
        "  ViT.eval()\n",
        "  test_loss = 0.0\n",
        "  correct = 0\n",
        "\n",
        "  # 13\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # 14\n",
        "          outputs = ViT(images)\n",
        "          predicted = torch.max(outputs, 1)[1]\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # 15\n",
        "          test_loss += loss.item()\n",
        "          correct += (labels == predicted).sum()\n",
        "  # 16\n",
        "  print(\n",
        "      f\"epoch {epoch+1} - test loss: {test_loss / len(test_loader):.4f}\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV48yLuAV1_n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
