{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "B6saS6tUtsch"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Patch Embedding\n",
        "# 0. Patch Embedding Variables\n",
        "p = 4 # patch\n",
        "w = 32 # width\n",
        "h = 32 # height\n",
        "c = 3 # channel\n",
        "b = 10 # batch\n",
        "d = 128 # Dim of patched embeddings\n",
        "cls = 10 # Class token size\n",
        "L = 8 # Transformer block size\n",
        "\n",
        "n = w//p"
      ],
      "metadata": {
        "id": "Hf3wuQVktvWc"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1g7cQbUuSUd",
        "outputId": "418d6725-5ba1-42e2-e0a1-6b72758b83b6"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainable Linear Projection이 필요\n",
        "# nn.Module로 구성\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.projection = nn.Linear(p*p*c, d) # These image patch vectors are now encoded using a linear transformation. Fixed size `d`\n",
        "\n",
        "    def patchify(self,img):\n",
        "      # Divide to patch\n",
        "      patched_img = img.view(b,c,h//p,p,w//p,p) # 이미지 1개당 N*N개 패치가 나오고, 패치 하나의 이미지는 P*P*C\n",
        "      patched_img = patched_img.transpose(3,4)\n",
        "      patched_img = patched_img.transpose(1,3)\n",
        "      patched_img = patched_img.transpose(1,2)\n",
        "      patched_img = patched_img.reshape(b,n*n,p*p*c)\n",
        "      return patched_img\n",
        "    def class_emb(self, patch):\n",
        "      x_class = nn.Parameter(torch.randn(10,1,d)).to(device)\n",
        "      with_class = torch.cat((x_class, patch), dim = 1)\n",
        "      # print(\"with class embedding : \", with_class.shape)\n",
        "      return with_class\n",
        "\n",
        "    def position_emb(self, class_patch):\n",
        "      pos_emb = nn.Parameter(torch.randn(10,n*n+1,d)).to(device)\n",
        "      with_class_pos = class_patch + pos_emb # 이게 맞나? 그냥 더하는게?\n",
        "      # print(\"with class & positional embedding : \", with_class_pos.shape)\n",
        "      return with_class_pos\n",
        "\n",
        "    def forward(self, x):\n",
        "      patched_ = self.patchify(x)\n",
        "      patched_ = self.projection(patched_)\n",
        "      patched_ = self.class_emb(patched_)\n",
        "      patched_ = self.position_emb(patched_)\n",
        "      return patched_"
      ],
      "metadata": {
        "id": "8zVvDkqouCUP"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "fNs7ITMQtlyZ"
      },
      "outputs": [],
      "source": [
        "# Transformer\n",
        "\n",
        "head_num = 8 # attention heads\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Attention, self).__init__()\n",
        "    self.w_q = nn.Parameter(torch.randn(d, n*n+1))\n",
        "    self.w_k = nn.Parameter(torch.randn(d, n*n+1))\n",
        "    self.w_v = nn.Parameter(torch.randn(d, n*n+1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # W_q,W_k,W_v 를 정의\n",
        "    q = x @ self.w_q\n",
        "    k = x @ self.w_k\n",
        "    v = x @ self.w_v\n",
        "    # QK^T를 만들기\n",
        "    qk_T = q @ k.T\n",
        "    # k의 차원 : D (Latent vector)\n",
        "    qk_T = qk_T / d\n",
        "    soft_ = nn.Softmax(dim = 0)\n",
        "    attention_ = soft_(qk_T)\n",
        "    # print(attention_.shape, v.shape)\n",
        "    ret = attention_ @ v\n",
        "    return ret\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.attn = Attention()\n",
        "    self.w_o = nn.Parameter(torch.randn(head_num*(n*n+1), d))\n",
        "  def forward(self,x):\n",
        "    # Head의 Concat이 필요\n",
        "    head_list = []\n",
        "    for h in range(head_num):\n",
        "\n",
        "      x_h = self.attn(x)\n",
        "      head_list.append(x_h)\n",
        "    ret = torch.cat(head_list, dim =1)\n",
        "    ret = ret @ self.w_o\n",
        "    return ret\n",
        "\n",
        "class VisionTransformerBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VisionTransformerBlock, self).__init__()\n",
        "    self.msa = MultiHeadAttention()\n",
        "    self.bn1 = nn.LayerNorm(d) # Size of BatchNorm1d is the input's size\n",
        "    self.bn2 = nn.LayerNorm(d) # Size of BatchNorm1d is the input's size\n",
        "    self.mlp = nn.Linear(d,d)\n",
        "  def forward(self, x):\n",
        "    # Batch Norm 1d\n",
        "    x = self.bn1(x)\n",
        "    # Multi-head Attention (Done)\n",
        "    x_attn = self.msa(x)\n",
        "    # print(x_attn.shape, x.shape)\n",
        "    # Residual connections\n",
        "    x_attn = x_attn + x\n",
        "    # Norm\n",
        "    out = self.bn2(x_attn)\n",
        "    # MLP\n",
        "    out = self.mlp(x_attn)\n",
        "    # Concat\n",
        "    out = out + x_attn\n",
        "    # print(out.shape)\n",
        "    return out\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VisionTransformer, self).__init__()\n",
        "    self.vit = nn.ModuleList([VisionTransformerBlock()\n",
        "                              for _ in range(L)])\n",
        "    # Full Connected Layer\n",
        "    self.mlp = nn.Sequential(\n",
        "            nn.LayerNorm(d),\n",
        "            nn.Linear(d, cls)\n",
        "        )\n",
        "    self.pe = PositionalEmbedding()\n",
        "  def forward(self,x):\n",
        "    pe_out = self.pe(x)\n",
        "    # Seqeuence L 반복\n",
        "    # ViT가 계속 업데이트 되야되는데 ..\n",
        "    outputs = []\n",
        "    for d in pe_out:\n",
        "      # print(d.shape)\n",
        "      for layer in self.vit:\n",
        "        d = layer(d)\n",
        "      # print(d.shape)\n",
        "      outputs.append(d)\n",
        "      # 각 이미지에 대한 output을 의미해야되는데\n",
        "      # label이 0,1이 아니라 1~10으로 구성이 되어 있다.\n",
        "    outputs = torch.stack(outputs,dim = 0).to(device)\n",
        "    out = self.mlp(outputs[:,0])\n",
        "    return out.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "ViT = VisionTransformer()\n",
        "ViT.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(ViT.parameters(), lr=0.001, momentum=0.9)\n",
        "# 지금은 patch를 1D로 만들고, cls, pos 를 붙임\n",
        "# patch , cls, pos를 붙인 다음에\n",
        "\n",
        "ViT.train()\n",
        "# print(ViT)\n",
        "for epoch in range(2):\n",
        "  for img, label in tqdm(train_loader):\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    out = ViT(img)\n",
        "    label_f32 = label.type('torch.LongTensor').to(device)\n",
        "    # print(out.dtype, label_f32.dtype)\n",
        "    loss = criterion(out, label_f32)\n",
        "\n",
        "    # loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() #retain_graph=True\n",
        "    optimizer.step()\n",
        "\n",
        "  ViT.eval()\n",
        "  test_loss = 0.0\n",
        "  correct = 0\n",
        "\n",
        "  # 13\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # 14\n",
        "          outputs = ViT(images)\n",
        "          predicted = torch.max(outputs, 1)[1]\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # 15\n",
        "          test_loss += loss.item()\n",
        "          correct += (labels == predicted).sum()\n",
        "  # 16\n",
        "  print(\n",
        "      f\"epoch {epoch+1} - test loss: {test_loss / len(test_loader):.4f}\"\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chDG9cMcuNo_",
        "outputId": "89de55f0-9d6b-4740-d84f-422866439b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 645/5000 [04:25<30:05,  2.41it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eV48yLuAV1_n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}